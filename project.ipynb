{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Enhanced Sketch2Scene using Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "\n",
    "Natural Language Processing and Computer vision are both the most actively developing machine learning research areas. Integrating these two fields has received a lot of attention recently. In this project, we would like to combine computer vision and Natural language Processing to transform an incomplete sketch into a scene. Translating a sketch into a scene is a challenging computer vision task. Realistic and meaningful scenes should contain multiple objects as well as a corresponding background, which is hard to extrapolate from a single input object. We propose a new approach to enhance the Sketch2Scene translation task using natural language processing techniques. Instead of directly transforming an image into another, we transform the sketch to a word and then use the word as a condition to create longer sentences and then use it to generate a scene. The first step is to classify the input sketch using a CNN based classifier. Once we have the class of the sketch, we find the most similar words related to that class. We generate a natural language scene description, conditioned on the class, and its similar words. Then we use a separate system that takes in the scene description and generates an image. Below is our scope of the project. \n",
    "\n",
    "  - Implement a new pipeline to generate realistic scenes from a single sketch\n",
    "  \n",
    "  - Show how NLP can improve the Sketch2Scene task with respect to quality and diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approach\n",
    "\n",
    "The pipeline in this project contains three main steps to take in an incomplete sketch and transform it into a complete scene. For the first step, we have a pre-trained [MobileNetV2](https://arxiv.org/abs/1801.04381) **sketch classifier** on ImageNet and fine-tuned it on 340 different object classes from the [Google QuickDraw dataset]. Given the input sketch, the output class of the classifier is used for the next step, which is the **caption generator**, to find the similar words and feed them into an encoder-decoder based sequential model to encode those words and use them to generate the scene captions. In the last step we infer the scenes using a pre-trained **Caption2Scene Generator** model which learns to retrieve objects and arrange them in the scene using the semantic relationship of the objects in the captions. Our main contribution is to experiment with two different baselines for this task :\n",
    "- **Baseline 1** - Get noun object from COCO captions and use it as the condition to generate captions \n",
    "- **Baseline 2** - Get similar words of the predicted class of the sketch, using word2Vec embedding and use as the condition to generate captions.\n",
    "\n",
    "Baseline 1 is used a sanity check to validate the conditional sequence genrative model which will be discussed in section 2.2 . And, for the second baseline we will use diferent methods like retorfitting , beam search and train time data augmentations to improve the generated captions.\n",
    "\n",
    "\n",
    " [Google QuickDraw dataset]:https://www.kaggle.com/c/quickdraw-doodle-recognition\n",
    " [Text2Scene]:http://openaccess.thecvf.com/content_CVPR_2019/papers/Tan_Text2Scene_Generating_Compositional_Scenes_From_Textual_Descriptions_CVPR_2019_paper.pdf\n",
    "![title](./images/diagram2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Sketch Classifier\n",
    "\n",
    "Since our inputs are hand-drawn sketches, we decided to use a pre-trained model from [Google QuickDraw Doodle Recognition Challenge](https://www.kaggle.com/c/quickdraw-doodle-recognition). This competition was released as an experimental game on sketches, and their dataset contains 50M drawings encompassing 340 label categories. For the first stage of our pipeline, we chose one of the [Pytorch Implementations](https://github.com/adam9500370/Kaggle-QuickDraw) from this challenge to work with. We didn't use the pre-trained model from this implementation. Instead, we made changes to the data loader and the train code to fit it for fine-tuning a Pytorch pre-trained model on the doodle dataset:\n",
    "\n",
    "\n",
    ">- changed the input size to 256*256\n",
    "- chnaged the last layer of the model\n",
    "- changed the normalization\n",
    "- fixed bugs in the original code\n",
    "\n",
    "We first tried to fine-tune the pre-trained [DenseNet](https://arxiv.org/abs/1608.06993) model, but training took more time than we expected, so we trained [**MobileNetV2**](https://arxiv.org/abs/1801.04381). We chose MobileNet because it is faster than DenseNet and it also maintain the accuracy.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    ">- epochs = 11\n",
    "- batch_size = 64\n",
    "- workers =2 \n",
    "- l_rate = 1e-3\n",
    "- weight_decay =1e-4\n",
    "- seed = 1234\n",
    "\n",
    "\n",
    "[Google QuickDraw Doodle Recognition Challenge]:https://www.kaggle.com/c/quickdraw-doodle-recognition\n",
    "[Pytorch Implementations]:https://github.com/adam9500370/Kaggle-QuickDraw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Caption Generator \n",
    "\n",
    "Based on [PyTorch Image Captiioning Tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning) code, we discarded the image encoder and attention module, and then added our keywords embeddings that are obtained from the object name of user sketch. We trained the decoder based on the keywords-caption pairs which we constructed from COCO caption dataset. The main challenge of this part is how to obtain better quality and more diverse captions conditioned on keywords. To takle this problem, we defined the baseline model and built up our model using some natural language technique that we have learned from the CMPT825 course. Also, we evaluated our outputs compared to the baseline using three metrics: BLEU4 score, Self-BLEU4 score, and Semantic Accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./CaptionGenerator\")\n",
    "os.chdir(\"./CaptionGenerator\")\n",
    "import json\n",
    "\n",
    "import utils\n",
    "from create_inputs import create_doodle_vocab, create_input_files, create_input_embeddings\n",
    "from train import Trainer\n",
    "import evaluation\n",
    "\n",
    "import inference\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Baseline 1\n",
    "\n",
    "- **[Step1] building keywords-caption pairs to generate Train/Valid/Test datasets**\\\n",
    "In COCO caption dataset, it has 5 captions per each image. So we sample **3 ~ 5 nouns of 5 captions as keywords** based on word frequency. For example, the baseline 1 model can ouput five different captions with respect to the single class name of \"castle\". As you can see, other keywords are different for each caption since we sample them using GloVe Embeddings. \n",
    "\n",
    ">['castle', 'cathedral', 'ruins'] a castle with a castle in the background \\\n",
    ">['castle', 'cave', 'tower'] a large castle with a castle in the background \\\n",
    ">['castle', 'hill', 'tower'] a castle with a castle on it \\\n",
    ">['castle', 'hill', 'kingdom'] a large hill with a castle in the background \\\n",
    ">['castle', 'fort', 'cathedral'] a large cathedral with a castle and a castle\n",
    "\n",
    "- **[Step2] training and validation** \\\n",
    "We initialize the hidden state of the decoder with the keywords embeddings, and calculate Cross Entropy Loss between the generated caption output and the ground truth caption. \n",
    "\n",
    "**Hyperparameters**\n",
    "For training we used the following hyperparameters:\n",
    "\n",
    ">- keyword_size=3 (5)\n",
    "- epochs=10\n",
    "- batch_size=64\n",
    "- workers=1 \n",
    "- decoder_lr=4e-4\n",
    "- embedding_dimension = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'man', 'bicycle'] a person is riding a bicycle but there is a train in the background\n",
      "['train', 'man', 'bicycle'] a guy that is riding his bike next to a train\n",
      "['train', 'man', 'bicycle'] a red and white train and a man riding a bicycle\n",
      "['train', 'man', 'bicycle'] a man riding a bike past a train traveling along tracks\n",
      "['train', 'man', 'bicycle'] a man on a bicycle riding next to a train\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "base_name = 'baseline'\n",
    "keyword_size = 3 # 5\n",
    "#############################\n",
    "#************takes  more than 3 hours to complete below *****************\n",
    "'''\n",
    "# [Step1] build Keywords-Caption pairs to generate Train/Valid/Test datasets \n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "create_input_files(base_name=base_name, keyword_size=keyword_size, caption_json_path='./data/dataset_coco.json')\n",
    "\n",
    "# [Step2] train and validate \n",
    "trainer = Trainer(base_name=base_name, keyword_size=keyword_size, \n",
    "                  epochs=20, batch_size=64, workers=1, decoder_lr=4e-4, \n",
    "                  checkpoint=None )\n",
    "trainer.run() \n",
    "'''\n",
    "#************************************************************************\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_KEYWORDS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    keywords = json.load(j)\n",
    "\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_CAPTIONS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    captions = json.load(j)\n",
    "    \n",
    "# Load word map (word2ix)\n",
    "with open(os.path.join('data', base_name, 'WORDMAP_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    word_map = json.load(j)  \n",
    "\n",
    "# print noun keyowrds \n",
    "for i in range(10,15): \n",
    "    print(utils.convert_idx2word(word_map, keywords[i]), ' '.join(utils.convert_idx2word(word_map, captions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Baseline 2\n",
    "\n",
    "- **[Step1] building Doodle Class Vocabulary** \\\n",
    "Since our model inference captions based on one of the Doodle class names, we require to build the training dataset related to Doodle class names. However, **Doodle has only 340 classes** so that we decide to use **GloVe Embeddings Similarity** to incease the size of vocabulary by **10 times**. Also, some class names that are not present in COCO caption vocabulary are discarded.\n",
    "\n",
    "\n",
    "- **[Step2] building keywords-caption pairs to generate Train/Valid/Test datasets**\\\n",
    "In COCO caption dataset, it has 5 captions per each image. So we sample 3 ~ 5 keywords from **the intersection of Doodle Class Vocabulary and COCO caption Vocabulary** based on word frequency. Based on the keyword size, some captions that don't have any similar keywords are removed (for keyword_size 3, 7985 of total 122919 samples are excluded). Compared to Baseline 1 ('train', 'man', and 'bike'), it samples different keywords ('riding', 'train', and 'bicycle') given the same captions. \n",
    "\n",
    ">['riding', 'train', 'bicycle'] a man on a bicycle riding next to a train \\\n",
    ">['riding', 'train', 'bicycle'] a man riding a bike past a train traveling along tracks \\\n",
    ">['riding', 'train', 'bicycle'] a person is riding a bicycle but there is a train in the background \\\n",
    ">['riding', 'train', 'bicycle'] a red and white train and a man riding a bicycle \\\n",
    ">['riding', 'train', 'bicycle'] a guy that is riding his bike next to a train\n",
    "\n",
    "\n",
    "- **[Step3] training and validation** \\\n",
    "We initialize the hidden state of the decoder with the keywords embeddings, and calculate Cross Entropy Loss between the generated caption output and the ground truth caption. \n",
    "\n",
    "**Hyperparameters**\n",
    "For training we used the following hyperparameters:\n",
    "\n",
    ">- keyword_size=3 (5)\n",
    "- epochs=10\n",
    "- batch_size=64\n",
    "- workers=1 \n",
    "- decoder_lr=4e-4\n",
    "- embedding_dimension = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['riding', 'train', 'bicycle'] a man on a bicycle riding next to a train\n",
      "['riding', 'train', 'bicycle'] a man riding a bike past a train traveling along tracks\n",
      "['riding', 'train', 'bicycle'] a person is riding a bicycle but there is a train in the background\n",
      "['riding', 'train', 'bicycle'] a red and white train and a man riding a bicycle\n",
      "['riding', 'train', 'bicycle'] a guy that is riding his bike next to a train\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "base_name = 'baseline2'\n",
    "keyword_size = 3 # 5\n",
    "#############################\n",
    "#************takes  more than 3 hours to complete  *****************\n",
    "'''\n",
    "# [Step1] build Doodle vocabulary by generating each 10 similar words per a class name \n",
    "create_doodle_vocab(w2v_magnitdue_path='data/glove.840B.300d.magnitude', \n",
    "                    wordmap_path='data/baseline/WORDMAP_coco_baseline_3.json', \n",
    "                    out_doodle_path='data/doodle_map.json',\n",
    "                    topn=10)\n",
    "# [Step2] build Keywords-Caption pairs to generate Train/Valid/Test datasets \n",
    "create_input_embeddings(base_name=base_name, keyword_size = keyword_size, \n",
    "                   caption_json_path='data/dataset_coco.json',\n",
    "                   doodle_json_path='data/doodle_map.json',\n",
    "                   w2v_magnitdue_path ='data/glove.42B.300d.magnitude')\n",
    "# [Step3] train and validate \n",
    "trainer = Trainer(base_name=base_name, keyword_size=keyword_size, \n",
    "                  epochs=20, batch_size=64, workers=1, decoder_lr=4e-4, \n",
    "                  checkpoint=None )\n",
    "trainer.run() \n",
    "'''\n",
    "#************************************************************************\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_KEYWORDS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    keywords = json.load(j)\n",
    "\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_CAPTIONS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    captions = json.load(j)\n",
    "    \n",
    "# Load word map (word2ix)\n",
    "with open(os.path.join('data', base_name, 'WORDMAP_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    word_map = json.load(j)  \n",
    "\n",
    "# print noun keyowrds \n",
    "for i in range(10,15): \n",
    "    print(utils.convert_idx2word(word_map, keywords[i]), ' '.join(utils.convert_idx2word(word_map, captions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Ours 1: Retrofitting \n",
    "- **[Step1] retrofitting GloVe Embeddings using COCO captions** \\\n",
    "The generated keywords from the baseline 2 might not be found in the vocabulary of COCO dataset. So our main idea is using retrofitting technique to improve the relations between keywords and captions for both training and inference. For this, we need to extract **noun object releationships from COCO dataset**, and then **retrofit GloVe embeddings** to find Q by minimizing the distance between word vectors in the relationship E (COCO Caption Dataset) as below equation. \n",
    "$$ L(Q)=\\sum_{i=1}^{n}\\begin{bmatrix}\\alpha_{i}||q_i - \\hat{q_i}||^2 + \\sum_{(i, j)\\in E}^{} \\beta_{i,j}||q_i - q_j||^2 \\end{bmatrix}$$\n",
    "We put $\\alpha = 1$ and $\\beta = 1$ and use the code from the HW2 with some modifications. We utilize this retrofitted embeddings instead of direct GloVe embeddings. \n",
    "\n",
    "\n",
    "- **[Step2] building Doodle Class Vocabulary** \\\n",
    "Since our model inference captions based on one of the Doodle class names, we require to build the training dataset related to Doodle class names. However, Doodle has only 340 classes so that we decide to use **Retrofitted GloVe Embeddings Similarity** to incease the size of vocabulary by **10 times**. Also, some class names that are not present in COCO caption vocabulary are discarded.\n",
    "\n",
    "\n",
    "- **[Step3] building keywords-caption pairs to generate Train/Valid/Test datasets**\\\n",
    "In COCO caption dataset, it has 5 captions per each image. So we sample 3 ~ 5 keywords from the intersection of Doodle Class Vocabulary and COCO caption Vocabulary based on word frequency. **Compared to Baselines, it generates similar to Baseline 1** which has keywords directly extracted from COCO captions rather than Baselin 2. Comparision is below. \n",
    "\n",
    "> Baseline 1 : ['train', 'man', 'bicycle'] \\\n",
    "> Baseline 2 : ['train', 'riding', 'bicycle'] \\\n",
    "> Retrofit   : ['train', 'man', 'bicycle']\n",
    "\n",
    "- **[Step4] training and validation** \\\n",
    "We initialize the hidden state of the decoder with the keywords embeddings, and calculate Cross Entropy Loss between the generated caption output and the ground truth caption. \n",
    "\n",
    "\n",
    "\n",
    "**Hyperparameters**\n",
    "For training we used the following hyperparameters:\n",
    "\n",
    ">- keyword_size=3 (5)\n",
    "- epochs=10\n",
    "- batch_size=64\n",
    "- workers=1 \n",
    "- decoder_lr=4e-4\n",
    "- embedding_dimension = 512\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'man', 'bicycle'] a man on a bicycle riding next to a train\n",
      "['train', 'man', 'bicycle'] a guy that is riding his bike next to a train\n",
      "['train', 'man', 'bicycle'] a red and white train and a man riding a bicycle\n",
      "['train', 'man', 'bicycle'] a person is riding a bicycle but there is a train in the background\n",
      "['train', 'man', 'bicycle'] a man riding a bike past a train traveling along tracks\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "base_name = 'retrofit'\n",
    "keyword_size = 3 # 5\n",
    "#############################\n",
    "#************takes more than 3 hours to complete *****************\n",
    "'''\n",
    "# [Step1] retrofit \n",
    "# create COCO Word Relation files \n",
    "create_relation(input_json='data/dataset_coco.json',\n",
    "                output_txt='data/coco-retrofitting.txt',\n",
    "                max_len=5)\n",
    "# conver w2v into magnitude file \n",
    "wordvecfile = os.path.join('data', 'glove.42B.300d')\n",
    "subprocess.run([\"python3\", \"-m\", \"pymagnitude.converter\", \n",
    "                \"-i\",wordvecfile+\".txt\", \"-o\", wordvecfile+\".magnitude\"], check=True) \n",
    "# retrofitting \n",
    "new_retro_file = os.path.join(\"data\", \"glove.42B.300d.retrofit.magnitude\")\n",
    "if not os.path.exists(new_retro_file):\n",
    "    # initialize retrofitting class\n",
    "    retro = Retrofitting(wv_magnitude_file=wordvecfile+\".magnitude\")\n",
    "    # read ontology files\n",
    "    coco = readLexicon(os.path.join(\"data\", \"coco-retrofitting.txt\"))\n",
    "    retro.retrofitting(coco, alpha=1, beta=1)\n",
    "\n",
    "    # write the final output into Magnitude format\n",
    "    retro.writeMagnitude_reduced(new_retro_file, 'data/coco/WORDMAP_coco_baseline_3.json')\n",
    "    subprocess.run([\"python3\", \"-m\", \"pymagnitude.converter\", \n",
    "            \"-i\",new_retro_file+\".txt\", \"-o\", new_retro_file], check=True)   \n",
    "\n",
    "# [Step2] build Doodle vocabulary by generating each 10 similar words per a class name \n",
    "create_doodle_vocab(w2v_magnitdue_path='data/glove.840B.300d.retrofit.magnitude', \n",
    "                    wordmap_path='data/baseline/WORDMAP_coco_baseline_3.json', \n",
    "                    out_doodle_path='data/doodle_map_retro.json',\n",
    "                    topn=10)\n",
    "# [Step3] build Keywords-Caption pairs to generate Train/Valid/Test datasets \n",
    "create_input_embeddings(base_name=base_name, keyword_size = keyword_size, \n",
    "                   caption_json_path='data/dataset_coco.json',\n",
    "                   doodle_json_path='data/doodle_map_retro.json',\n",
    "                   w2v_magnitdue_path ='data/glove.42B.300d.retrofit.magnitude')\n",
    "# [Step4] train and validate  \n",
    "trainer = Trainer(base_name=base_name, keyword_size=keyword_size, \n",
    "                  epochs=20, batch_size=64, workers=1, decoder_lr=4e-4, \n",
    "                  checkpoint=None )\n",
    "trainer.run() \n",
    "'''\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_KEYWORDS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    keywords = json.load(j)\n",
    "\n",
    "# Load encoded captions \n",
    "with open(os.path.join('data', base_name, 'TEST_CAPTIONS_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    captions = json.load(j)\n",
    "    \n",
    "# Load word map (word2ix)\n",
    "with open(os.path.join('data', base_name, 'WORDMAP_coco_{}_{}.json'.format(base_name, keyword_size)), 'r') as j:\n",
    "    word_map = json.load(j)  \n",
    "\n",
    "# print noun keyowrds \n",
    "for i in range(10,15): \n",
    "    print(utils.convert_idx2word(word_map, keywords[i]), ' '.join(utils.convert_idx2word(word_map, captions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 Ours 2: Retrofitting + Data Augmentation\n",
    "\n",
    "To increase the diversity of the model, we try to apply **online data augmentation technique** by randomly shuffling keywords among \"keywords+2\" pairs at each step during training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "base_name = 'augment'\n",
    "keyword_size = 3 # 5\n",
    "#############################\n",
    "#************takes more than 3 hours to complete *****************\n",
    "'''\n",
    "create_input_embeddings(base_name=base_name, keyword_size = keyword_size+2, \n",
    "                   caption_json_path='data/dataset_coco.json',\n",
    "                   doodle_json_path='data/doodle_map_retro.json',\n",
    "                   w2v_magnitdue_path ='data/glove.42B.300d.retrofit.magnitude')    \n",
    "    \n",
    "trainer = Trainer(base_name=base_name, keyword_size=keyword_size, \n",
    "                epochs=20, batch_size=64, workers=1, decoder_lr=4e-4, \n",
    "                checkpoint=None )\n",
    "trainer.run()\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 Ours 1+ or 2+: Retrofitting | Data Augmentation + Beam Search\n",
    "\n",
    "To improve the performance of our model, we add Beam Search, and **find the optimal beam size as 3** for both keyword size 3 and 5. We use **Normalized Log Probability** to count scores and skip the caption when its length is less than 5. Since our HW4 didn't give us better score, instead we implemented based on [PyTorch Image Captiioning Tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning).  \n",
    "\n",
    ">When keyword_size = 3, \\\n",
    "Test BLEU Score (beam size: 1): 0.36034294511596937 \\\n",
    "**Test BLEU Score (beam size: 3): 0.3659191816566126** \\\n",
    "Test BLEU Score (beam size: 5): 0.36262749823432455 \\\n",
    "Test BLEU Score (beam size: 7): 0.358677671380397 \n",
    "\n",
    ">When keyword_size = 5, \\\n",
    "Test BLEU Score (beam size: 1): 0.40214758432644454 \\\n",
    "**Test BLEU Score (beam size: 3): 0.42702580300893117** \\\n",
    "Test BLEU Score (beam size: 5): 0.4237561271804396 \\\n",
    "Test BLEU Score (beam size: 7): 0.4180983371130172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************takes  more than 10 min to complete  *****************\n",
    "'''\n",
    "base_name = 'retrofit'\n",
    "keyword_size = 3 # 5\n",
    "for b in [1, 3, 5, 7]: \n",
    "    # Evaluation on Test split \n",
    "    checkpoint = os.path.join('pretrained','BEST_checkpoint_coco_{}_{}.pth.tar'.format(base_name, keyword_size))\n",
    "    references, hypotheses, ref_inwords, hyp_inwords, key_inwords = evaluation.evaluate(\n",
    "        base_name, keyword_size, checkpoint, beam_size = b)\n",
    "    print(\"Test BLEU Score (beam size: {}): {}\".format(b, corpus_bleu(references, hypotheses)))\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Caption2Scene Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part , we used a pretrained model proposed by the paper [Text2Scene: Generating Compositional Scenes from Textual Descriptions](https://arxiv.org/pdf/1809.01110.pdf).\n",
    "Text2Scene is a model that generates various forms of compositional scene representations from natural language descriptions.Unlike many recent works , they do not use Generative Adversial Netwrok (GAN) for this task.Text2Scene instead learns to sequentially generate objects and their attributes (location, size, appearance, etc) at every time step by attending to different parts of the input text and the current status of the generated scene .They show that under minor modifications, the proposed framework can handle the generation of different forms of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images . That is the reason why we chose this model to convert our captions to scene which will satusfy our motivation to generate realistic looking images which can be used as a dataset to improve object detection and image captioning tasks.We used the pretrained model that generates composite scenes using COCO dataset , as we trained our caption generator based on the same dataset. \n",
    "\n",
    "As you can see from the below image , this model uses the caption to retrieve objects and position them using the relationship between objects in the caption recursively to generate the final scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/000000162.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data\n",
    "- [Google QuickDraw dataset](https://www.kaggle.com/c/quickdraw-doodle-recognition) Used for sketch classifier\n",
    "    - 4,960,414 train sketches\n",
    "    - 102,000 valiation sketches\n",
    "    - 112,199 test sketches\n",
    "- [COCO Dataset](http://cocodataset.org/#home)\n",
    "    - Captioning 2015 dataset for caption generation\n",
    "    - Detection 2017 dataset to biuld database for Caption2Scene inference \n",
    "- [Word2vec embedding](https://nlp.stanford.edu/projects/glove/)\n",
    "    - Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip used for searching keywords to generate captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Experimental Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Classifier Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After two days training for 11 epochs on 4,960,414 train images with batch size of 64 we got the following accuracy on 102,000 validation images for top 1,2 and 3 output classes by comparing it with the ground truth:\n",
    "\n",
    " - top1_accuracy = 74.989 \n",
    " - top2_accuracy = 86.136 \n",
    " - **top3_accuracy = 89.997**\n",
    " \n",
    "\n",
    "Since for the test file , we dont have the ground truth , we uploaded the outputs to the kaggle competition to get a unbaised evaluation of our model\n",
    "Test accuracy -  **.86517** in the competition :\n",
    "\n",
    " ![](./images/test3_1.png)\n",
    "\n",
    "\n",
    "Below are some samples from our validation images and their top 3 output classes:\n",
    "\n",
    " ![](./images/radio2.png)\n",
    " ![](./images/fire2.png)\n",
    " ![](./images/hockey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Caption Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate Our Models(1-3) compared to two baselines **Qualitatively** and **Quantitatively**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.2.1 Qualitative Evalutation \n",
    "\n",
    "Since our goal is to generate diverse captions given a single doodle class name, we write the inference code to produce five different captions by sampling keywords everytime (but we fix the first keywrod as Doodle class name). We compare all the outputs from four different models (Baseline 1 and 2, and Ours 1 and 2) for the 'castle' Doodle class input. \n",
    "\n",
    "- Both **Our methods produces more diverse keywords and captions** compared to both Baselines which generates duplicate keywords and captions. \n",
    "\n",
    "- Both **Our methods** used the COCO retrofitted word embeddings, so the generated keywords showed **more connection to the generated captions**. \n",
    "\n",
    "- **Ours 2 showed more diversity than Ours 1** since Ours 2 is trained with the data augmentation.\n",
    "\n",
    "- Since the keywords of **Baseline 1 and 2** are sampled using GloVe embeddings, **most keywords shows very similar semantic meanings** of input class name such as 'palace'. \n",
    "\n",
    "- **Baseline 1** is trained on nouns-caption pairs so that it only takes 'castle' keywords to generate captions, resulting in gerating **more duplicated captions**. \n",
    "\n",
    "\n",
    ">### Baseline 1 (nouns of COCO)\n",
    "['castle', 'cathedral', 'ruins'] a castle with a castle in the background \\\n",
    "['castle', 'mansion', 'palace'] a castle with a castle in the background \\\n",
    "['castle', 'medieval', 'hill'] a castle with a castle in the background \\\n",
    "['castle', 'hill', 'ruins'] a castle with a mountain in the background  \\\n",
    "['castle', 'hill', 'ruins'] a castle with a mountain in the background\n",
    "\n",
    ">### Baseline 2 (GloVe embedding)\n",
    "['castle', 'medieval', 'mansion'] a castle with a castle in the background \\\n",
    "['castle', 'ruins', 'mansion'] a castle with a castle in the background \\\n",
    "['castle', 'ruins', 'palace'] a large castle like building with a clock on it \\\n",
    "['castle', 'palace', 'cathedral'] a large castle like structure with a castle in the background \\\n",
    "['castle', 'cathedral', 'tower'] a castle with a castle in the background\n",
    "\n",
    ">### Ours 1 (retrofitting with COCO)\n",
    "['castle', 'gate', 'forest'] a large castle with a sign on it \\\n",
    "['castle', 'park', 'town'] a group of people in a town square \\\n",
    "['castle', 'park', 'street'] a street sign in front of a building \\\n",
    "['castle', 'forest', 'clock'] a large castle with a clock on it \\\n",
    "['castle', 'inside', 'park'] a group of people inside of a building\n",
    "\n",
    ">### Ours 2 (retro + data augmentation) \n",
    "['castle', 'view', 'cave'] a view of a castle with a castle in the background \\\n",
    "['castle', 'courtyard', 'tower'] a castle like building with a clock tower in the background \\\n",
    "['castle', 'door', 'city'] a castle with a clock on the side of it \\\n",
    "['castle', 'park', 'hill'] a castle with a large clock on the side of it \\\n",
    "['castle', 'battle', 'inside'] a castle with a large castle in the background \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['castle', 'cave', 'tower'] a castle like building with a clock on it\n",
      "['castle', 'cave', 'close'] a close up of a castle with a castle in the background\n",
      "['castle', 'wall', 'forest'] a castle with a clock on the side of it\n",
      "['castle', 'part', 'front'] a castle with a castle in the background\n",
      "['castle', 'inside', 'gate'] a large castle with a clock on the side of it\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Inference outputs\n",
    "############################\n",
    "doodle_class = \"castle\"\n",
    "\n",
    "# base_name, keyword_size = 'baseline', 3 # baseline2\n",
    "# w2v_magnitdue_path = 'data/glove.42B.300d.magnitude'\n",
    "\n",
    "base_name, keyword_size = 'augment', 3 \n",
    "w2v_magnitdue_path = 'data/glove.42B.300d.retrofit.magnitude'\n",
    "\n",
    "# Sketch2Caption Inference\n",
    "keys, sentences, failure = inference.sketch2caption(doodle_class=doodle_class, \n",
    "                checkpoint='pretrained/BEST_checkpoint_coco_{}_{}.pth.tar'.format(base_name, keyword_size), \n",
    "                word_map_path='data/{}/WORDMAP_coco_{}_{}.json'.format(base_name, base_name, keyword_size), \n",
    "                w2v_magnitdue_path=w2v_magnitdue_path, \n",
    "                beam_size=3, num_sen=5)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    print(keys[i], ' '.join(sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Quantitative Evalutation \n",
    "\n",
    "Our task is not only generating good captions but also diverse outputs given single class name. Also, the output should provide meaningful captions related to the keywords including class name. Thus, we evaluate our results using the below metrics based on the goals. \n",
    "\n",
    "- Quality: BLEU4\n",
    "- Diversity: [Self-BLEU4](https://arxiv.org/pdf/1802.01886.pdf)\n",
    "- Semantic Accuracy: check if keywords are present in the output captions or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.1 Quality: BLEU4 score \n",
    "We evaluate the quality of the generated captions based on the ground truths in Test Dataset. We average BLEU1-4 with the same weights using [NLTK corpus_bleu()](https://www.nltk.org/_modules/nltk/translate/bleu_score.html) method. \n",
    "\n",
    "- **Ours 1 showed the best quality of output captions for both keyword size**\n",
    "- Ours 2 gave the lowest accuracy. We assume that data augmentation of keywords might disturb the model to learn how to generate captions based on the ground truth. \n",
    "\n",
    "\n",
    ">When Keyword size is 3, \\\n",
    "Baseline 1  : 0.35447955769733747 \\\n",
    "Baseline 2  : 0.329850170252854 \\\n",
    "    **Ours 1  : 0.36034294511596937** \\\n",
    "    Ours 2  : 0.3328501542643379\n",
    "\n",
    ">When Keyword size is 5, \\\n",
    "Baseline 1 : 0.3872948174606526 \\\n",
    "Baseliee 2 : 0.3584256187411019 \\\n",
    "    **Ours 1 : 0.40214758432644454** \\\n",
    "    Ours 2 : 0.22522079844728965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# BLEU4 Evaluation \n",
    "############################\n",
    "# base_name, keyword_size = 'baseline', 3 # 5\n",
    "# base_name, keyword_size = 'baseline2',3 # 5\n",
    "base_name, keyword_size = 'retrofit', 3 # 5\n",
    "# base_name, keyword_size = 'augment', 3 # 5\n",
    "\n",
    "# Evaluation on Test split \n",
    "checkpoint = os.path.join('pretrained','BEST_checkpoint_coco_{}_{}.pth.tar'.format(base_name, keyword_size))\n",
    "references, hypotheses, ref_inwords, hyp_inwords, key_inwords = evaluation.evaluate(base_name, keyword_size, checkpoint)\n",
    "print(\"Test BLEU Score: \", corpus_bleu(references, hypotheses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.2 Diversity: Self-BLEU4 score \n",
    "To evaluate how diverse the caption is, we use the diversity metric, called as [Self-BLEU4](https://arxiv.org/pdf/1802.01886.pdf). Since our model inference five captions, we implemented Self-BLEU4 by calculating BLEU4 scores between each caption and the correspoding four captions. We add all scores for all Doodle class names and normalize them. Since the Self-BLEU4 score represents diversity, **lower score is better**. \n",
    "\n",
    "- **Ours 1 showed the best diversity of output captions for both keyword size** (lowest score) \n",
    "- We expected that the Ours 2 would show better diversity but it gave the worst diversity compared to other methods. As we mentioned in the quality evaluation, we believe that data augmentation doesn't help to guide the network to learn. \n",
    "\n",
    "\n",
    ">When Keyword size is 3, \\\n",
    "Baseline 1  : 0.3989175388159808 \\\n",
    "Baseline 2  :  0.37840215178057673 \\\n",
    "    **Ours 1  : 0.2909877677336257** \\\n",
    "    Ours 2  : 0.44158844645520334\n",
    "\n",
    ">When Keyword size is 5, \\\n",
    "Baseline 1 : 0.2599512809602099 \\\n",
    "Baseliee 2 : 0.4420841167665019 \\\n",
    "    **Ours 1 : 0.2460393290983978** \\\n",
    "    Ours 2 : 0.5398446699047991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Bleu 0.3989175388159808\n",
      "Self-Bleu 0.37840215178057673\n",
      "Self-Bleu 0.2909877677336257\n",
      "Self-Bleu 0.44158844645520334\n",
      "Self-Bleu 0.2599512809602099\n",
      "Self-Bleu 0.4420841167665019\n",
      "Self-Bleu 0.2460393290983978\n",
      "Self-Bleu 0.5398446699047991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5398446699047991"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Self-BLEU4 Evaluation \n",
    "############################\n",
    "evaluation.selfbleu('baseline', 3)\n",
    "evaluation.selfbleu('baseline2', 3)\n",
    "evaluation.selfbleu('retrofit', 3)\n",
    "evaluation.selfbleu('augment', 3)\n",
    "\n",
    "evaluation.selfbleu('baseline', 5)\n",
    "evaluation.selfbleu('baseline2', 5)\n",
    "evaluation.selfbleu('retrofit', 5)\n",
    "evaluation.selfbleu('augment', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.3 Semantic Accuracy\n",
    "Our method propose the caption generation based on keywords, so it is important to evaluate if keywords are present in the corresponding ouput captions. We calculate semantic accuracy against each input keywords for all Doodle class names, and normalize them. \n",
    "\n",
    "- **Ours 1 showed the best semantic accuracy of output captions for both keyword size** \n",
    "- Baseline 1 showed similar accuracy with Ours 2 because it is trained on COCO caption dataset only wihout any Word2Vec embeddings\n",
    "\n",
    ">When Keyword size is 3, \\\n",
    "Baseline 1  :0.6058997050147493 \\\n",
    "Baseline 2  :  0.56379821958457 \\\n",
    "    **Ours 1  : 0.7125382262996944** \\\n",
    "    Ours 2  : 0.6515337423312887\n",
    "\n",
    ">When Keyword size is 5, \\\n",
    "Baseline 1 : 0.7525525525525532 \\\n",
    "Baseliee 2 : 0.5384146341463412 \\\n",
    "    **Ours 1 : 0.7576687116564423** \\\n",
    "    Ours 2 : 0.22585034013605473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy 0.6058997050147493\n",
      "Semantic accuracy 0.56379821958457\n",
      "Semantic accuracy 0.7125382262996944\n",
      "Semantic accuracy 0.6515337423312887\n",
      "Semantic accuracy 0.7525525525525532\n",
      "Semantic accuracy 0.5384146341463412\n",
      "Semantic accuracy 0.7576687116564423\n",
      "Semantic accuracy 0.22585034013605473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22585034013605473"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Self-BLEU4 Evaluation \n",
    "############################\n",
    "evaluation.semantic_acc_keys('baseline', 3)\n",
    "evaluation.semantic_acc_keys('baseline2', 3)\n",
    "evaluation.semantic_acc_keys('retrofit', 3)\n",
    "evaluation.semantic_acc_keys('augment', 3)\n",
    "\n",
    "evaluation.semantic_acc_keys('baseline', 5)\n",
    "evaluation.semantic_acc_keys('baseline2', 5)\n",
    "evaluation.semantic_acc_keys('retrofit', 5)\n",
    "evaluation.semantic_acc_keys('augment', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Caption2Scene Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.1 Inception score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception score was fist introduced by [Salimans et al](https://arxiv.org/pdf/1606.03498.pdf) to evaluate the images generated by a generative model . In this method , we apply the [inception model](https://arxiv.org/abs/1409.4842) to every generated image to get the conditional label distribution $p(y|x)$ , where $y$ is the target data and $x$ is generated data.Images that contain meaningful objects should have a conditional label distribution $p(y|x)$ with low entropy. Moreover, the model is also expected to generate varied images, so the marginal $\\int p(y|x = G(z))dz$ should have high entropy. Combining these two requirements, the metric that we propose is: $exp(E_xKL(p(y|x)||p(y)))$, where the exponentiated results makes the values comparable.This inception score is directly provided by pytorch library and we used this score to evaluate the quality of our generated scenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/nlpclass-1197-g-thefinalproject/project\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"./Text2Scene\")\n",
    "print(os.getcwd())\n",
    "os.chdir(\"./Text2Scene\")\n",
    "from evaluate_images import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score evaluation for image quality\n",
      "=============================================\n",
      "Evaluating Images generated using caption generated by the baseline method\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/nlpclass-1197-g-thefinalproject/project/Text2Scene/evaluate_images.py:181: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x).data.cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score for Baseline images  47.763093829189415\n",
      "=============================================\n",
      "Evaluating Images generated using caption generated by our method\n",
      "=================================================================\n",
      "Inception Score for Retrofitted images  49.08888457356971\n"
     ]
    }
   ],
   "source": [
    "print(\"Inception Score evaluation for image quality\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "print(\"Evaluating Images generated using caption generated by the baseline method\")\n",
    "print(\"==========================================================================\")\n",
    "inc_score = evaluate_inception(images_path = \"./logs/baseline/composites_samples\")\n",
    "print(\"Inception Score for Baseline images \",inc_score)\n",
    "\n",
    "\n",
    "print(\"Evaluating Images generated using caption generated by our method\")\n",
    "print(\"=================================================================\")\n",
    "inc_score = evaluate_inception(images_path = \"./logs/retrofit/composites_samples\")\n",
    "print(\"Inception Score for Retrofitted images \",inc_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inception Score evaluation for image quality**\n",
    "\n",
    "Evaluating Images generated using caption generated by the **baseline method**\n",
    "Inception Score for Baseline images  47.763093829189415\n",
    "\n",
    "Evaluating Images generated using caption generated by **our method**\n",
    "Inception Score for Retrofitted images  49.08888457356971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.2 Semantic Object Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Generative networks conditioned on simple textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image synthesis models is still challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address the aforementioned challenges ,[Semantic Object Accuracy for Generative Text-to-Image Synthesis](https://arxiv.org/pdf/1910.13321.pdf) introduces both a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used this Sematic Obejct Accuracy (SOA) to evaluate the accuracy of our generated images. We used a pretrained YOLOv3 network to infer the objects in the scene . Since we dont have the ground truth object for our generated captions , we used NLTK.pos_tag to get noun words in our captions.We noticed that some objects in the captions are not a defined class for detection , so we used wordnet corpus to find the hypernyms of a every objects in the class to get all parents of the objects in the captions . Then we intersected these hypernyms and objects with predefined detection classes to generate ground truth for our captions . TO calcuate accuracy, we intersected these generated ground truth and yolov3 predictions sets to get count of caption2scene detections and divided it by total caption groundtruth to all generated images and averaged it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Object Accuracy (SOA) evaluation for caption to ima ge accuracy\n",
      "Evaluating Images generated using captions generated by the baseline method\n",
      "Loading network.....\n",
      "Network successfully loaded\n",
      "Using GPU: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1405/1405 [00:19<00:00, 72.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOA Score for Baseline images  0.48320413436692505\n",
      "Evaluating Images generated using captions generated by the retrofitted method\n",
      "Loading network.....\n",
      "Network successfully loaded\n",
      "Using GPU: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1430/1430 [00:19<00:00, 74.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOA Score for Baseline images  0.22536287242169595\n"
     ]
    }
   ],
   "source": [
    "print(\"Semantic Object Accuracy (SOA) evaluation for caption to ima ge accuracy\")\n",
    "print(\"Evaluating Images generated using captions generated by the baseline method\")\n",
    "\n",
    "pred_dict = yolo_predictions(images = \"./logs/baseline/composites_samples\")\n",
    "soa_accuracy = cal_soa_accuracy(images_path=\"./logs/baseline/composites_samples\",caption_path='./examples/composites_samples_baseline.json',yolo_pred_dict=pred_dict)\n",
    "print(\"SOA Score for Baseline images \",soa_accuracy)\n",
    "\n",
    "print(\"Evaluating Images generated using captions generated by the retrofitted method\")\n",
    "pred_dict_retro = yolo_predictions(images=\"./logs/retrofit/composites_samples\")\n",
    "soa_accuracy = cal_soa_accuracy(images_path=\"./logs/retrofit/composites_samples\",\n",
    "                            caption_path='./examples/composites_samples_retorfit.json', yolo_pred_dict=pred_dict_retro)\n",
    "print(\"SOA Score for Baseline images \", soa_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Object Accuracy (SOA) evaluation for caption to ima ge accuracy\n",
    "\n",
    "Evaluating Images generated using captions generated by the baseline method\n",
    "\n",
    "Loading network.....\n",
    "\n",
    "Network successfully loaded\n",
    "\n",
    "Using GPU: 0\n",
    "\n",
    "**SOA Score for Baseline images  0.48320413436692505**\n",
    "\n",
    "Evaluating Images generated using captions generated by the retrofitted method\n",
    "\n",
    "Loading network.....\n",
    "\n",
    "Network successfully loaded\n",
    "\n",
    "Using GPU: 0\n",
    "\n",
    "**SOA Score for Baseline images  0.22536287242169595**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 .Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Sketch Classifier:\n",
    "\n",
    "we chose one of the [Pytorch Implementations](https://github.com/adam9500370/Kaggle-QuickDraw) from this challenge to work with. We didn't use the pre-trained model from this implementation. Instead, we made changes to the data loader and the train code to fit it for fine-tuning a Pytorch pre-trained model on the doodle dataset:\n",
    "\n",
    "\n",
    ">- changed the input size to 256*256\n",
    "- chnaged the last layer of the model\n",
    "- changed the normalization\n",
    "- fixed bugs in the original code\n",
    "\n",
    "We first tried to fine-tune the pre-trained [DenseNet](https://arxiv.org/abs/1608.06993) model, but training took more time than we expected, so we trained [**MobileNetV2**](https://arxiv.org/abs/1801.04381). We chose MobileNet because it is faster than DenseNet and it also maintain the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Caption Generator:\n",
    "\n",
    "- For the generative sequence model we used [pytorch image captioning tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning) as reference.\n",
    "- For retrofitting , we used the code that we wrote from HW2.\n",
    "- All other code to preprocess , inference and evaluation are written by us ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Caption2Scene Generator:\n",
    "\n",
    "- The code and pretrained model are downloaded from the [text2scene](git@github.com:uvavision/Text2Scene.git) official pytorch repository and made some minor modifications to work with it.\n",
    "- The evaluation code for inception score and Semantic object accuracy were written by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results\n",
    "\n",
    "## Some generated captions from Baseline 1, Baseline 2, retrofitting and retrofitting + data augmentation\n",
    "\n",
    ">### Baseline 1 (nouns of COCO)\n",
    "['castle', 'cathedral', 'ruins'] a castle with a castle in the background \\\n",
    "['castle', 'mansion', 'palace'] a castle with a castle in the background \\\n",
    "['castle', 'medieval', 'hill'] a castle with a castle in the background \\\n",
    "['castle', 'hill', 'ruins'] a castle with a mountain in the background  \\\n",
    "['castle', 'hill', 'ruins'] a castle with a mountain in the background\n",
    "\n",
    ">### Baseline 2 (GloVe embedding)\n",
    "['castle', 'medieval', 'mansion'] a castle with a castle in the background \\\n",
    "['castle', 'ruins', 'mansion'] a castle with a castle in the background \\\n",
    "['castle', 'ruins', 'palace'] a large castle like building with a clock on it \\\n",
    "['castle', 'palace', 'cathedral'] a large castle like structure with a castle in the background \\\n",
    "['castle', 'cathedral', 'tower'] a castle with a castle in the background\n",
    "\n",
    ">### Ours 1 (retrofitting with COCO) \n",
    "['castle', 'gate', 'forest'] a large castle with a sign on it \\\n",
    "['castle', 'park', 'town'] a group of people in a town square \\\n",
    "['castle', 'park', 'street'] a street sign in front of a building \\\n",
    "['castle', 'forest', 'clock'] a large castle with a clock on it \\\n",
    "['castle', 'inside', 'park'] a group of people inside of a building\n",
    "\n",
    ">### Ours 2 (retro + data augmentation) \n",
    "['castle', 'view', 'cave'] a view of a castle with a castle in the background \\\n",
    "['castle', 'courtyard', 'tower'] a castle like building with a clock tower in the background \\\n",
    "['castle', 'door', 'city'] a castle with a clock on the side of it \\\n",
    "['castle', 'park', 'hill'] a castle with a large clock on the side of it \\\n",
    "['castle', 'battle', 'inside'] a castle with a large castle in the background \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Final **good looking images** from captions generated by Retrofitting(2.2.3):\n",
    "\n",
    "\n",
    "![](./images/000000034.png)\n",
    "\n",
    "![](./images/000000135.png)\n",
    "\n",
    "![](./images/000000150.png)\n",
    "\n",
    "![](./images/000000155.png)\n",
    "\n",
    "![](./images/000000162.png)\n",
    "\n",
    "![](./images/000000167.png)\n",
    "\n",
    "![](./images/000000178.png)\n",
    "\n",
    "![](./images/000000205.png)\n",
    "\n",
    "![](./images/000000207.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Failure cases** from captions generated by Retrofitting(2.2.3):\n",
    "![](./images/000000000.png)\n",
    "\n",
    "![](./images/000000001.png)\n",
    "\n",
    "![](./images/000000002.png)\n",
    "\n",
    "![](./images/000000004.png)\n",
    "\n",
    "![](./images/000000010.png)\n",
    "\n",
    "![](./images/000000220.png)\n",
    "\n",
    "![](./images/000000241.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analysis of the Results (Conclusion) \n",
    "\n",
    "![](./images/Quantitative_Evaluation.png )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method (Retrofit-3 and 5) produces high quality **(High BLEU4 score)** and more diverse **(Lower Self-BLEU4 score)** captions given sketch input. \n",
    "Our method generates captions which have better correspondence with input sketch **(Higher Semantic Accuracy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the experimental setup section , baseline -1 is trained completely using COCO caption and so it is not fair to compare baseline-1 with other methods, but the results are displayed for your information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to improve diversity by randomly selecting similar words to input words as a data augmentation during training . Although it showed better results for training data , it couldn't generalise well for test data as you can see from the above results. We suspect that since we replace the actual word with similar word , the link between input and output are broken , which led to reduced score . For future work , we want to try replacing the word in both input and output to get better results ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/scene_evaluation.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method generates better captions resulting in better scenes **(higher Inception score)**. Caption2Scene model generates the scenes with more diverse objects and relationships, which are unseen by YOLOv3 detector **(Lower SOA score)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Limitations\n",
    "\n",
    "- As you can see from the failure cases from the output images ,whenever **\"a group of people\"** is present in the caption , the text2scene focuses only on them and adds more and more people and discards other objects and relationships in the caption .\n",
    "- We can also see from the SOA score of scenes generated using retrofitted model , the captions generated are too complex and diverse for a retreival network to handle . Maybe using a GAN model to generate images may help in these cases but it may drop the quality of generated images.\n",
    "- Our pipeline depends on the class relationships between doodle classes(340) and coco classes (91) which add more unseen objects in the captions. As the Text2Scene model is trained using COCO dataset , it cannot identify thse missing classes like Eiffel tower , Monalisa ,Camel etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Future Work \n",
    "\n",
    "- We can extend this model to produce additional data to improve object detection and image captioning task\n",
    "  \n",
    "- The end-to-end architecture will help to produce more corresponding scenes with input sketch with respect to pose and style of objects\n",
    "  \n",
    "- Conditional GAN can replace our conditional sequence generator to improve the diversity of captions \n",
    "\n",
    "- input sketch classifer can be trained using RNN to learn the sketching style which is available in the google quick draw dataset and improve the classifier accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
